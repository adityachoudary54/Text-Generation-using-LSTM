{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Harry Potter final.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "FRLgrlazcvLH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a1c4689f-e73e-4b86-d992-8fe961b0a899"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "CtVRxmJfc4OZ",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9ce068e0-63cf-4898-d6fd-7e04102ee515"
      },
      "cell_type": "code",
      "source": [
        "files.upload()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-25be31b3-ff4a-4207-9a51-33b7f59d6cba\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-25be31b3-ff4a-4207-9a51-33b7f59d6cba\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "F4s5TvzLdanl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Open File and read contents"
      ]
    },
    {
      "metadata": {
        "id": "pUY-mlWwdSw7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "6373ee9d-2e0f-4db7-c693-d6bcc91dfcb0"
      },
      "cell_type": "code",
      "source": [
        "doc=open('part1.txt')\n",
        "text=doc.read()\n",
        "print(text[:200])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Harry Potter and the Sorcerer's Stone \n",
            "\n",
            "CHAPTER ONE \n",
            "\n",
            "THE BOY WHO LIVED \n",
            "\n",
            "Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. Th\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "q33tVEA1d_NU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing and Cleaning"
      ]
    },
    {
      "metadata": {
        "id": "z8jx80mDdel5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import string\n",
        "tokens=text.split()\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "tokens = [w.translate(table) for w in tokens]\n",
        "tokens = [word for word in tokens if word.isalpha()]\n",
        "tokens = [word.lower() for word in tokens]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sn6VnrWaeD8J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "total_len=50+1\n",
        "sequence_list=list()\n",
        "for i in range(total_len,len(tokens)):\n",
        "  seq=tokens[i-total_len:i]\n",
        "  line=' '.join(seq)\n",
        "  sequence_list.append(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1YaMDMI_o30d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Fit all text in tokenizer for using word embeddings"
      ]
    },
    {
      "metadata": {
        "id": "Jy0p5Dd8otX2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer=Tokenizer()\n",
        "tokenizer.fit_on_texts(sequence_list)\n",
        "sequences=tokenizer.texts_to_sequences(sequence_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oqZH7-TSo-L0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vocab_size=len(tokenizer.word_index)+1 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fVBJbJm8qNe0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Make X and Y to fit into Model"
      ]
    },
    {
      "metadata": {
        "id": "jNFpKLXIpt3O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sequences=np.array(sequences)\n",
        "X=sequences[:,:-1]\n",
        "Y=sequences[:,-1]\n",
        "Y=to_categorical(Y,num_classes=vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uqbkpL6Wp3Ad",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cdeea7db-64b4-4580-e15a-7cd9ad69dcde"
      },
      "cell_type": "code",
      "source": [
        "seq_length=X.shape[1]\n",
        "print(seq_length)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "I27qaW0KqsOy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Define Model"
      ]
    },
    {
      "metadata": {
        "id": "mm3AUKynqg53",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "12f97df9-230e-4110-a1b4-b1492f033d09"
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
        "model.add(LSTM(250, return_sequences=True))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(250, activation='relu'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 50, 50)            296950    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 50, 250)           301000    \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 100)               140400    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 250)               25250     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 5939)              1490689   \n",
            "=================================================================\n",
            "Total params: 2,254,289\n",
            "Trainable params: 2,254,289\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tq2a3s1frVTN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1717
        },
        "outputId": "a26fac47-c085-4ab6-dd4a-3efbe65e46eb"
      },
      "cell_type": "code",
      "source": [
        "model.fit(X, Y, batch_size=128, epochs=50)\n",
        "model.save('model.h5')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "75088/75088 [==============================] - 145s 2ms/step - loss: 6.7164 - acc: 0.0466\n",
            "Epoch 2/50\n",
            "75088/75088 [==============================] - 144s 2ms/step - loss: 6.3483 - acc: 0.0513\n",
            "Epoch 3/50\n",
            "75088/75088 [==============================] - 146s 2ms/step - loss: 6.1545 - acc: 0.0621\n",
            "Epoch 4/50\n",
            "75088/75088 [==============================] - 146s 2ms/step - loss: 6.0114 - acc: 0.0680\n",
            "Epoch 5/50\n",
            "75088/75088 [==============================] - 145s 2ms/step - loss: 5.8826 - acc: 0.0779\n",
            "Epoch 6/50\n",
            "75088/75088 [==============================] - 144s 2ms/step - loss: 5.7587 - acc: 0.0866\n",
            "Epoch 7/50\n",
            "75088/75088 [==============================] - 144s 2ms/step - loss: 5.6511 - acc: 0.0953\n",
            "Epoch 8/50\n",
            "75088/75088 [==============================] - 142s 2ms/step - loss: 5.5616 - acc: 0.0994\n",
            "Epoch 9/50\n",
            "75088/75088 [==============================] - 143s 2ms/step - loss: 5.4602 - acc: 0.1041\n",
            "Epoch 10/50\n",
            "75088/75088 [==============================] - 143s 2ms/step - loss: 5.3875 - acc: 0.1070\n",
            "Epoch 11/50\n",
            "75088/75088 [==============================] - 144s 2ms/step - loss: 5.3389 - acc: 0.1119\n",
            "Epoch 12/50\n",
            "75088/75088 [==============================] - 144s 2ms/step - loss: 5.2448 - acc: 0.1164\n",
            "Epoch 13/50\n",
            "75088/75088 [==============================] - 143s 2ms/step - loss: 5.1591 - acc: 0.1203\n",
            "Epoch 14/50\n",
            "75088/75088 [==============================] - 145s 2ms/step - loss: 5.0848 - acc: 0.1224\n",
            "Epoch 15/50\n",
            "75088/75088 [==============================] - 147s 2ms/step - loss: 5.0135 - acc: 0.1259\n",
            "Epoch 16/50\n",
            "75088/75088 [==============================] - 147s 2ms/step - loss: 4.9745 - acc: 0.1286\n",
            "Epoch 17/50\n",
            "75088/75088 [==============================] - 148s 2ms/step - loss: 4.9446 - acc: 0.1285\n",
            "Epoch 18/50\n",
            "75088/75088 [==============================] - 147s 2ms/step - loss: 4.8757 - acc: 0.1308\n",
            "Epoch 19/50\n",
            "75088/75088 [==============================] - 146s 2ms/step - loss: 4.8002 - acc: 0.1337\n",
            "Epoch 20/50\n",
            "75088/75088 [==============================] - 147s 2ms/step - loss: 4.7317 - acc: 0.1370\n",
            "Epoch 21/50\n",
            "75088/75088 [==============================] - 147s 2ms/step - loss: 4.6637 - acc: 0.1412\n",
            "Epoch 22/50\n",
            "75088/75088 [==============================] - 147s 2ms/step - loss: 4.6060 - acc: 0.1436\n",
            "Epoch 23/50\n",
            "75088/75088 [==============================] - 147s 2ms/step - loss: 4.7052 - acc: 0.1383\n",
            "Epoch 24/50\n",
            "75088/75088 [==============================] - 148s 2ms/step - loss: 4.6367 - acc: 0.1416\n",
            "Epoch 25/50\n",
            "75088/75088 [==============================] - 147s 2ms/step - loss: 4.5583 - acc: 0.1460\n",
            "Epoch 26/50\n",
            "75088/75088 [==============================] - 148s 2ms/step - loss: 4.4974 - acc: 0.1490\n",
            "Epoch 27/50\n",
            "75088/75088 [==============================] - 148s 2ms/step - loss: 4.4417 - acc: 0.1539\n",
            "Epoch 28/50\n",
            "75088/75088 [==============================] - 147s 2ms/step - loss: 4.3856 - acc: 0.1567\n",
            "Epoch 29/50\n",
            "75088/75088 [==============================] - 146s 2ms/step - loss: 4.3313 - acc: 0.1611\n",
            "Epoch 30/50\n",
            "75088/75088 [==============================] - 149s 2ms/step - loss: 4.2782 - acc: 0.1653\n",
            "Epoch 31/50\n",
            "75088/75088 [==============================] - 148s 2ms/step - loss: 4.2275 - acc: 0.1704\n",
            "Epoch 32/50\n",
            "75088/75088 [==============================] - 147s 2ms/step - loss: 4.1751 - acc: 0.1738\n",
            "Epoch 33/50\n",
            "75088/75088 [==============================] - 148s 2ms/step - loss: 4.1285 - acc: 0.1780\n",
            "Epoch 34/50\n",
            "75088/75088 [==============================] - 150s 2ms/step - loss: 4.0802 - acc: 0.1833\n",
            "Epoch 35/50\n",
            "75088/75088 [==============================] - 150s 2ms/step - loss: 4.0345 - acc: 0.1876\n",
            "Epoch 36/50\n",
            "75088/75088 [==============================] - 150s 2ms/step - loss: 3.9907 - acc: 0.1912\n",
            "Epoch 37/50\n",
            "75088/75088 [==============================] - 149s 2ms/step - loss: 3.9441 - acc: 0.1972\n",
            "Epoch 38/50\n",
            "75088/75088 [==============================] - 149s 2ms/step - loss: 3.8992 - acc: 0.2021\n",
            "Epoch 39/50\n",
            "75088/75088 [==============================] - 145s 2ms/step - loss: 3.8610 - acc: 0.2057\n",
            "Epoch 40/50\n",
            "75088/75088 [==============================] - 147s 2ms/step - loss: 3.8159 - acc: 0.2126\n",
            "Epoch 41/50\n",
            "75088/75088 [==============================] - 148s 2ms/step - loss: 3.7771 - acc: 0.2154\n",
            "Epoch 42/50\n",
            "75088/75088 [==============================] - 148s 2ms/step - loss: 3.7345 - acc: 0.2206\n",
            "Epoch 43/50\n",
            "75088/75088 [==============================] - 148s 2ms/step - loss: 3.6934 - acc: 0.2271\n",
            "Epoch 44/50\n",
            "75088/75088 [==============================] - 149s 2ms/step - loss: 3.6530 - acc: 0.2307\n",
            "Epoch 45/50\n",
            "75088/75088 [==============================] - 149s 2ms/step - loss: 3.6127 - acc: 0.2374\n",
            "Epoch 46/50\n",
            "75088/75088 [==============================] - 147s 2ms/step - loss: 3.5708 - acc: 0.2416\n",
            "Epoch 47/50\n",
            "75088/75088 [==============================] - 148s 2ms/step - loss: 3.5297 - acc: 0.2464\n",
            "Epoch 48/50\n",
            "75088/75088 [==============================] - 147s 2ms/step - loss: 3.4911 - acc: 0.2524\n",
            "Epoch 49/50\n",
            "75088/75088 [==============================] - 147s 2ms/step - loss: 3.4519 - acc: 0.2575\n",
            "Epoch 50/50\n",
            "75088/75088 [==============================] - 147s 2ms/step - loss: 3.4114 - acc: 0.2634\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hN8YJGt0FWBD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1717
        },
        "outputId": "e9cda7b7-fe68-44e3-f786-79a14362a2f9"
      },
      "cell_type": "code",
      "source": [
        "model.fit(X, Y, batch_size=128, epochs=50)\n",
        "model.save('model2.h5')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "75088/75088 [==============================] - 147s 2ms/step - loss: 3.3682 - acc: 0.2711\n",
            "Epoch 2/50\n",
            "75088/75088 [==============================] - 147s 2ms/step - loss: 3.3273 - acc: 0.2759\n",
            "Epoch 3/50\n",
            "75088/75088 [==============================] - 147s 2ms/step - loss: 3.2861 - acc: 0.2822\n",
            "Epoch 4/50\n",
            "75088/75088 [==============================] - 147s 2ms/step - loss: 3.2468 - acc: 0.2886\n",
            "Epoch 5/50\n",
            "75088/75088 [==============================] - 148s 2ms/step - loss: 3.2054 - acc: 0.2945\n",
            "Epoch 6/50\n",
            "75088/75088 [==============================] - 147s 2ms/step - loss: 3.1650 - acc: 0.3002\n",
            "Epoch 7/50\n",
            "75088/75088 [==============================] - 146s 2ms/step - loss: 3.1188 - acc: 0.3087\n",
            "Epoch 8/50\n",
            "75088/75088 [==============================] - 147s 2ms/step - loss: 3.0853 - acc: 0.3130\n",
            "Epoch 9/50\n",
            "75088/75088 [==============================] - 147s 2ms/step - loss: 3.0406 - acc: 0.3191\n",
            "Epoch 10/50\n",
            "75088/75088 [==============================] - 147s 2ms/step - loss: 2.9937 - acc: 0.3285\n",
            "Epoch 11/50\n",
            "75088/75088 [==============================] - 147s 2ms/step - loss: 2.9564 - acc: 0.3340\n",
            "Epoch 12/50\n",
            "75088/75088 [==============================] - 148s 2ms/step - loss: 2.9131 - acc: 0.3413\n",
            "Epoch 13/50\n",
            "75088/75088 [==============================] - 148s 2ms/step - loss: 2.8728 - acc: 0.3479\n",
            "Epoch 14/50\n",
            "75088/75088 [==============================] - 147s 2ms/step - loss: 2.8284 - acc: 0.3564\n",
            "Epoch 15/50\n",
            "75088/75088 [==============================] - 147s 2ms/step - loss: 2.7881 - acc: 0.3621\n",
            "Epoch 16/50\n",
            "75088/75088 [==============================] - 147s 2ms/step - loss: 2.7447 - acc: 0.3719\n",
            "Epoch 17/50\n",
            "75088/75088 [==============================] - 147s 2ms/step - loss: 2.7021 - acc: 0.3799\n",
            "Epoch 18/50\n",
            "75088/75088 [==============================] - 148s 2ms/step - loss: 2.6560 - acc: 0.3862\n",
            "Epoch 19/50\n",
            "75088/75088 [==============================] - 147s 2ms/step - loss: 2.6171 - acc: 0.3933\n",
            "Epoch 20/50\n",
            "75088/75088 [==============================] - 144s 2ms/step - loss: 2.5741 - acc: 0.4020\n",
            "Epoch 21/50\n",
            "75088/75088 [==============================] - 147s 2ms/step - loss: 2.5338 - acc: 0.4080\n",
            "Epoch 22/50\n",
            "75088/75088 [==============================] - 147s 2ms/step - loss: 2.4930 - acc: 0.4169\n",
            "Epoch 23/50\n",
            "75088/75088 [==============================] - 147s 2ms/step - loss: 2.4556 - acc: 0.4259\n",
            "Epoch 24/50\n",
            "75088/75088 [==============================] - 148s 2ms/step - loss: 2.4137 - acc: 0.4317\n",
            "Epoch 25/50\n",
            "75088/75088 [==============================] - 149s 2ms/step - loss: 2.3662 - acc: 0.4403\n",
            "Epoch 26/50\n",
            "75088/75088 [==============================] - 147s 2ms/step - loss: 2.3311 - acc: 0.4482\n",
            "Epoch 27/50\n",
            "75088/75088 [==============================] - 148s 2ms/step - loss: 2.2850 - acc: 0.4577\n",
            "Epoch 28/50\n",
            "75088/75088 [==============================] - 146s 2ms/step - loss: 2.2408 - acc: 0.4668\n",
            "Epoch 29/50\n",
            "75088/75088 [==============================] - 146s 2ms/step - loss: 2.2046 - acc: 0.4733\n",
            "Epoch 30/50\n",
            "75088/75088 [==============================] - 146s 2ms/step - loss: 2.1723 - acc: 0.4807\n",
            "Epoch 31/50\n",
            "75088/75088 [==============================] - 146s 2ms/step - loss: 2.1318 - acc: 0.4881\n",
            "Epoch 32/50\n",
            "75088/75088 [==============================] - 146s 2ms/step - loss: 2.0848 - acc: 0.4986\n",
            "Epoch 33/50\n",
            "75088/75088 [==============================] - 146s 2ms/step - loss: 2.0494 - acc: 0.5046\n",
            "Epoch 34/50\n",
            "75088/75088 [==============================] - 144s 2ms/step - loss: 2.0153 - acc: 0.5139\n",
            "Epoch 35/50\n",
            "75088/75088 [==============================] - 146s 2ms/step - loss: 1.9758 - acc: 0.5205\n",
            "Epoch 36/50\n",
            "75088/75088 [==============================] - 147s 2ms/step - loss: 1.9337 - acc: 0.5312\n",
            "Epoch 37/50\n",
            "75088/75088 [==============================] - 148s 2ms/step - loss: 1.9005 - acc: 0.5379\n",
            "Epoch 38/50\n",
            "75088/75088 [==============================] - 148s 2ms/step - loss: 1.8664 - acc: 0.5441\n",
            "Epoch 39/50\n",
            "75088/75088 [==============================] - 148s 2ms/step - loss: 1.8204 - acc: 0.5552\n",
            "Epoch 40/50\n",
            "75088/75088 [==============================] - 148s 2ms/step - loss: 1.7901 - acc: 0.5618\n",
            "Epoch 41/50\n",
            "75088/75088 [==============================] - 148s 2ms/step - loss: 1.7523 - acc: 0.5691\n",
            "Epoch 42/50\n",
            "75088/75088 [==============================] - 148s 2ms/step - loss: 1.7257 - acc: 0.5745\n",
            "Epoch 43/50\n",
            "75088/75088 [==============================] - 148s 2ms/step - loss: 1.6852 - acc: 0.5843\n",
            "Epoch 44/50\n",
            "75088/75088 [==============================] - 148s 2ms/step - loss: 1.6538 - acc: 0.5906\n",
            "Epoch 45/50\n",
            "75088/75088 [==============================] - 148s 2ms/step - loss: 1.6206 - acc: 0.5987\n",
            "Epoch 46/50\n",
            "75088/75088 [==============================] - 148s 2ms/step - loss: 1.5834 - acc: 0.6076\n",
            "Epoch 47/50\n",
            "75088/75088 [==============================] - 148s 2ms/step - loss: 1.5486 - acc: 0.6159\n",
            "Epoch 48/50\n",
            "75088/75088 [==============================] - 148s 2ms/step - loss: 1.5232 - acc: 0.6205\n",
            "Epoch 49/50\n",
            "75088/75088 [==============================] - 147s 2ms/step - loss: 1.4894 - acc: 0.6283\n",
            "Epoch 50/50\n",
            "75088/75088 [==============================] - 148s 2ms/step - loss: 1.4552 - acc: 0.6351\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KLdNc8kMkabo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "files.download('model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7qx15sh-rWFq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Predicting Text"
      ]
    },
    {
      "metadata": {
        "id": "4LUPIuybq7UP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
        "  result = list()\n",
        "  in_text = seed_text\n",
        "  for _ in range(n_words):\n",
        "    encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "    encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "    yhat = model.predict_classes(encoded, verbose=0)\n",
        "    out_word = ''\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "      if index == yhat:\n",
        "        out_word = word\n",
        "        break\n",
        "    in_text += ' ' + out_word\n",
        "    result.append(out_word)\n",
        "  return ' '.join(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P3hOdP0Gkkfn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "d44baa29-b65e-43ca-f4ce-00012b7b2e69"
      },
      "cell_type": "code",
      "source": [
        "from random import randint\n",
        "seed_text = sequence_list[randint(0,len(sequence_list))]\n",
        "print(seed_text)\n",
        " \n",
        "# generate new text\n",
        "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
        "print(generated)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "its eyes to the ceiling it gave harry a look that said quite plainly i get that all the time i know harry murmured through the glass though he wasnt sure the snake could hear him it must be really annoying the snake nodded vigorously where do you come from anyway\n",
            "well dumbledores only nicolas flamel we believed to cry the same dream you didnt have scars burning at his side malfoy looked at it and said harry quirrell telling him i dont understand why i thought you werent going to be changing them around the potters youd better let you\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "grPzsZDkmNyz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Final prediction<br>\n",
        "\n",
        "its eyes to the ceiling it gave harry a look that said quite plainly i get that all the time i know harry murmured through the glass though he wasnt sure the snake could hear him it must be really annoying the snake nodded vigorously where do you come from anyway\n",
        "well dumbledores only nicolas flamel we believed to cry the same dream you didnt have scars burning at his side malfoy looked at it and said harry quirrell telling him i dont understand why i thought you werent going to be changing them around the potters youd better let you"
      ]
    },
    {
      "metadata": {
        "id": "OpOwWe3xlnnl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}